{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24dfefe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of toxicity data:\n",
      "Number of rows: 171\n",
      "Number of columns: 1204\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Perform basic dimension check\n",
    "\n",
    "#Read in full toxicity dataset\n",
    "data = pd.read_csv('toxicity_data.csv')\n",
    "\n",
    "#Output dimensions of the data\n",
    "print(\"Dimensions of toxicity data:\")\n",
    "print(\"Number of rows:\", data.shape[0])\n",
    "print(\"Number of columns:\", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2456fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec28d751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid function test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_sigmoid():\n",
    "    #Sample case\n",
    "    ]\n",
    "    test_inputs = [0, 1, -1, 10, -10]\n",
    "    expected_outputs = [0.5, 0.73105858, 0.26894142, 0.9999546, 4.53978687e-05]\n",
    "    for input_val, expected_output in zip(test_inputs, expected_outputs):\n",
    "        output = sigmoid(input_val)\n",
    "        assert np.isclose(output, expected_output), f\"Test failed: Input {input_val}, Expected {expected_output}, Got {output}\"\n",
    "\n",
    "    print(\"Sigmoid function test passed.\")\n",
    "test_sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "790d20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, learning_rate=0.001, n_iters=1000):\n",
    "    \"\"\"\n",
    "    Fit logistic regression model to the data.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialize parameters\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    # Gradient descent\n",
    "    for _ in range(n_iters):\n",
    "        # Linear combination of weights and features, plus bias\n",
    "        linear_model = np.dot(X, weights) + bias\n",
    "        # Apply sigmoid function\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "\n",
    "        # Compute gradients\n",
    "        dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))  # Derivative w.r.t weights\n",
    "        db = (1 / n_samples) * np.sum(y_predicted - y)         # Derivative w.r.t bias\n",
    "\n",
    "        # Update parameters\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "def logistic_regression_predict(X, weights, bias):\n",
    "    \"\"\"\n",
    "    Predict using logistic regression model.\n",
    "    \"\"\"\n",
    "    linear_model = np.dot(X, weights) + bias\n",
    "    y_predicted = sigmoid(linear_model)\n",
    "    y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "    return np.array(y_predicted_cls)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def select_best_feature(X, y, selected_features):\n",
    "    remaining_features = [i for i in range(X.shape[1]) if i not in selected_features]\n",
    "    best_score = -np.inf\n",
    "    best_feature = None\n",
    "\n",
    "    for feature in remaining_features:\n",
    "        X_temp = X[:, selected_features + [feature]]\n",
    "        weights, bias, _ = logistic_regression(X_temp, y)\n",
    "\n",
    "        linear_model = np.dot(X_temp, weights) + bias\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "\n",
    "        score = compute_score(y, y_predicted)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_feature = feature\n",
    "\n",
    "    return best_feature\n",
    "\n",
    "def compute_score(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e8b68b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute score function test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_compute_score():\n",
    "    #Test cases\n",
    "    y_true = np.array([0, 1, 0, 1, 1])\n",
    "    y_pred = np.array([0, 1, 0, 0, 1])\n",
    "    expected_output = 0.8\n",
    "\n",
    "    #Run\n",
    "    output = compute_score(y_true, y_pred)\n",
    "    assert np.isclose(output, expected_output), f\"Test failed: Expected {expected_output}, Got {output}\"\n",
    "\n",
    "    print(\"Compute score function test passed.\")\n",
    "test_compute_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62d77b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features for logistic regression (X):\n",
      "   MATS3v  nHBint10  MATS3s  MATS3p  nHBDon_Lipinski  minHBint8  MATS3e  \\\n",
      "0  0.0908         0  0.0075  0.0173                0        0.0 -0.0436   \n",
      "1  0.0213         0  0.1144 -0.0410                0        0.0  0.1231   \n",
      "2  0.0018         0 -0.0156 -0.0765                2        0.0 -0.1138   \n",
      "3 -0.0251         0 -0.0064 -0.0894                3        0.0 -0.0747   \n",
      "4  0.0135         0  0.0424 -0.0353                0        0.0 -0.0638   \n",
      "\n",
      "   MATS3c  minHBint2  MATS3m  ...   WTPT-3   WTPT-4   WTPT-5  ETA_EtaP_L  \\\n",
      "0  0.0409        0.0  0.1368  ...   0.0000   0.0000   0.0000      0.1780   \n",
      "1 -0.0316        0.0  0.1318  ...  28.2185   8.8660  19.3525      0.1739   \n",
      "2 -0.1791        0.0  0.0615  ...  33.1064   5.2267  27.8796      0.1688   \n",
      "3 -0.1151        0.0  0.0361  ...  32.5232   7.7896  24.7336      0.1702   \n",
      "4  0.0307        0.0  0.0306  ...  32.0726  12.3240  19.7486      0.1789   \n",
      "\n",
      "   ETA_EtaP_F  ETA_EtaP_B  nT5Ring  SHdNH  ETA_dEpsilon_C  MDEO-22  \n",
      "0      1.5488      0.0088        0    0.0         -0.0868     0.00  \n",
      "1      1.3718      0.0048        2    0.0         -0.0810     0.25  \n",
      "2      1.4395      0.0116        2    0.0         -0.1004     0.00  \n",
      "3      1.4654      0.0133        2    0.0         -0.1010     0.00  \n",
      "4      1.4495      0.0120        2    0.0         -0.1071     0.00  \n",
      "\n",
      "[5 rows x 1203 columns]\n",
      "Target variable for logistic regression:\n",
      "0    NonToxic\n",
      "1    NonToxic\n",
      "2    NonToxic\n",
      "3    NonToxic\n",
      "4    NonToxic\n",
      "Name: Class, dtype: object\n",
      "Target variable for logistic regression:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#Toxicity data has class variable (toxic vs nontoxic) as target variable\n",
    "X = data.drop('Class', axis=1) \n",
    "y = data['Class']\n",
    "\n",
    "#Print XPortion\n",
    "print(\"Input features for logistic regression (X):\")\n",
    "print(X.head())\n",
    "#Print YPortion\n",
    "print(\"Target variable for logistic regression:\")\n",
    "print(y.head())\n",
    "\n",
    "#Convert target variable numeric\n",
    "# Assuming 'y' is your array of strings containing 'NonToxic' or 'Toxic'\n",
    "y_numeric = [0 if label == 'NonToxic' else 1 for label in y]\n",
    "\n",
    "#Print YNumericPortion\n",
    "print(\"Target variable for logistic regression:\")\n",
    "print(y_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4675335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for splitting data into training/testing sections\n",
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"Split the dataset into training and testing sets.\"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    #Shuffle indices\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    #Calculate the number of samples for the testing set\n",
    "    num_test_samples = int(len(X) * test_size)\n",
    "\n",
    "    #Split the shuffled indices into training and testing indices\n",
    "    test_indices = indices[:num_test_samples]\n",
    "    train_indices = indices[num_test_samples:]\n",
    "\n",
    "    #Split the dataset into training and testing sets\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4216fc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (137, 1203)\n",
      "Shape of X_test: (34, 1203)\n",
      "Shape of y_train: (137,)\n",
      "Shape of y_test: (34,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
    "#Test train/test split\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a51dce3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m accuracy_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_iterations \u001b[38;5;129;01min\u001b[39;00m num_iterations_list:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#Training step\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     weights, bias \u001b[38;5;241m=\u001b[39m logistic_regression(X_train, y_train, learning_rate, num_iterations)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#Make predictions on the test data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m predict(X_test, weights, bias)\n",
      "Cell \u001b[1;32mIn[25], line 19\u001b[0m, in \u001b[0;36mlogistic_regression\u001b[1;34m(X, y, learning_rate, n_iters)\u001b[0m\n\u001b[0;32m     16\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m sigmoid(linear_model)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m dw \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m n_samples) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT, (y_predicted \u001b[38;5;241m-\u001b[39m y))  \u001b[38;5;66;03m# Derivative w.r.t weights\u001b[39;00m\n\u001b[0;32m     20\u001b[0m db \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m n_samples) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y_predicted \u001b[38;5;241m-\u001b[39m y)         \u001b[38;5;66;03m# Derivative w.r.t bias\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "#Switch hyperparameters up\n",
    "learning_rate = 0.001\n",
    "num_iterations_list = [100]\n",
    "\n",
    "#Train the logistic regression model and evaluate accuracy for each number of iterations\n",
    "accuracy_scores = []\n",
    "for num_iterations in num_iterations_list:\n",
    "    #Training step\n",
    "    weights, bias = logistic_regression(X_train, y_train, learning_rate, num_iterations)\n",
    "    \n",
    "    #Make predictions on the test data\n",
    "    y_pred = predict(X_test, weights, bias)\n",
    "    \n",
    "    #Calculate accuracy\n",
    "    acc = accuracy(y_test, y_pred)\n",
    "    accuracy_scores.append(acc)\n",
    "\n",
    "#Plot accuracy over iterations\n",
    "plt.plot(num_iterations_list, accuracy_scores, marker='o')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Logistic Regression Model')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc66ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
